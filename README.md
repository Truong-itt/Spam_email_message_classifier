# ğŸ§  **Spam email Classification**

## ğŸ“‹ **OVERVIEW**
This project involves building and evaluating a **K-Nearest Neighbors (KNN)** model for classification tasks. The notebook includes:

- ğŸ“Š **Data Preprocessing**
- ğŸ‹ï¸â€â™‚ï¸ **Model Training**
- ğŸ§ª **Performance Evaluation**
- ğŸ“ˆ **Visualization of Results**

## ğŸ” **DESCRIPTION**
The main steps include data processing and data understanding, then performing **tf-idf** normalization into vector data, and then performing testing on many popular classification models.
**Decision Tree**, **XGBClassifier**, **RandomForest**, **AdaBoost**, **KNeighbors**, **GradientBoosting**, **Stacking**, **Voting**
### ğŸ’¡ **KNN (K-Nearest Neighbors)**
 - Supervised learning algorithm used for classification and regression.
 - It predicts based on the majority vote of its k-nearest data points.
 - Simple but sensitive to noisy data and requires proper scaling.
### ğŸ’¡ **Decision Tree**
 - Uses a tree-like model of decisions, splitting data based on features.
 - Easy to interpret, but can overfit if not pruned properly.
 - Works well with small to medium-sized datasets.
### ğŸ’¡ **Random Forest**
 - Ensemble model that combines multiple Decision Trees.
 - Reduces overfitting by averaging the predictions of individual trees.
 - Robust, accurate, and effective for a wide range of data.
### ğŸ’¡ **XGBoost (Extreme Gradient Boosting)**
 - Powerful ensemble model using boosting with Decision Trees.
 - Optimized for speed and performance, often used in competitions.
 - Needs careful tuning to avoid overfitting.
### ğŸ’¡ **AdaBoost (Adaptive Boosting)**
 - Focuses on correcting errors made by previous models in the ensemble.
 - Adjusts the weights of misclassified samples in each iteration.
 - Effective but sensitive to noisy data.
### ğŸ’¡ **Gradient Boosting**
 - Boosting algorithm that minimizes prediction error using Gradient Descent.
 - Builds trees sequentially, each correcting the errors of the previous.
 - High accuracy but can be computationally expensive.
### ğŸ’¡ **Stacking Classifier**
 - Ensemble method that combines multiple models using a meta-learner.
 - Each base model's predictions are used as input features for the final model.
 - Can leverage the strengths of diverse models for improved accuracy.
### ğŸ’¡ **Voting Classifier**
- Combines predictions from different models using majority voting.
- Supports "hard voting" (majority class) and "soft voting" (probability-based).
- Simple yet effective ensemble approach for boosting model performance.
Open the Jupyter Notebook and execute the cells sequentially.


## ğŸ›  **DEPENDENCIES**

This project requires the following Python packages:

- ğŸ **Python 3.x**
- ğŸ“’ **Jupyter Notebook**
- ğŸ—ƒï¸ **Pandas** - Data manipulation
- ğŸ”¢ **NumPy** - Numerical operations
- ğŸ–¼ï¸ **Matplotlib** - Basic plotting
- ğŸ“Š **Seaborn** - Enhanced visualizations
- ğŸ“š **NLTK** - Natural Language Processing
- ğŸ“ **Regex** - Text preprocessing
- ğŸ’¬ **WordCloud** - Text visualization

### Machine Learning
- ğŸŒ³ **DecisionTree** - Tree-based model
- ğŸ¤– **RandomForest** - Ensemble of trees
- ğŸ“ˆ **LogisticRegression** - Linear classifier
- ğŸ˜ï¸ **KNN** - Nearest neighbors
- ğŸ”¥ **XGBoost** - Gradient boosting
- âš¡ **AdaBoost** - Adaptive boosting
- ğŸš€ **GradientBoosting** - Sequential boosting
- ğŸ—³ï¸ **VotingClassifier** - Majority voting
- ğŸ”— **StackingClassifier** - Meta-ensemble

### Preprocessing
- ğŸ”  **TfidfVectorizer** - Text vectorization
- ğŸ”¢ **LabelEncoder** - Encoding labels
- ğŸ“ **MinMaxScaler** - Feature scaling

### Evaluation
- ğŸ“Š **ConfusionMatrix** - Error analysis
- ğŸ“ˆ **ROC Curve** - Model performance
- ğŸ“‹ **Classification Report** - Metrics summary

## ğŸ“ **USAGE**

To explore and use this project, follow the link below:

- ğŸ“ˆ **View and Run the Notebook on Kaggle**:

  [Spam Message Classification Notebook on Kaggle](https://www.kaggle.com/code/hduytrng/spam-message)

Simply click the link above to access the full Jupyter Notebook directly on Kaggle, where you can view the code, run it, and interact with the data.


## ğŸ“Š **RESULTS**
The KNN model's performance is evaluated using metrics such as:
- **Accuracy**
- **Precision**
- **Confusion Matrix**

The results are visualized with plots, and a detailed discussion is included in the notebook.

## ğŸ“„ **LICENSE**
This project is licensed under the **MIT License** - see the LICENSE file for details.

## ğŸŒ **REFERENCES**
## ğŸŒ **REFERENCES**
- [K-Nearest Neighbors Algorithm Explained](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)
- [Decision Tree Algorithm Explained](https://en.wikipedia.org/wiki/Decision_tree_learning)
- [Random Forest Algorithm Explained](https://en.wikipedia.org/wiki/Random_forest)
- [XGBoost Documentation](https://xgboost.readthedocs.io/en/stable/)
- [AdaBoost Algorithm Explained](https://en.wikipedia.org/wiki/AdaBoost)
- [Gradient Boosting Algorithm Explained](https://en.wikipedia.org/wiki/Gradient_boosting)
- [Stacking Classifier in Scikit-Learn](https://scikit-learn.org/stable/modules/ensemble.html#stacking)
- [Voting Classifier in Scikit-Learn](https://scikit-learn.org/stable/modules/ensemble.html#voting-classifier)
- [Logistic Regression Algorithm Explained](https://en.wikipedia.org/wiki/Logistic_regression)
- [Confusion Matrix and Performance Metrics](https://en.wikipedia.org/wiki/Confusion_matrix)
- [TF-IDF Vectorization Explained](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)

